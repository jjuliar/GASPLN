# -*- coding: utf-8 -*-
"""Base_For_IronyDetection_Albertina.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MWiM6e-Yp4-Xm2vxbgsQBwJcshJsE0gZ
"""

# Transformers installation

# !pip install --upgrade accelerate

# !pip install evaluate

# !pip install datasets

# !pip install sklearn

import numpy as np
import evaluate
import gdown
import pandas as pd
import torch

from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification
from transformers import TrainingArguments, Trainer
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

"""## Loading dataset"""

# imdb = load_dataset("imdb")
url = "https://drive.google.com/file/d/1klgcUpRhAwhxnxHTWGhRJrAh3n_ZButf/view?usp=share_link"
output = "treino_tweets.csv"
gdown.download(url, output, quiet=False, fuzzy=True)

url2 = "https://drive.google.com/file/d/1386svaKYa2_b3zPLBjmOzIVbOLH4ZurY/view?usp=sharing"
output2 = "test_tweets.csv"
gdown.download(url2, output2, quiet=False, fuzzy=True)

#Test
from datasets import Dataset, DatasetDict

# train_dataset = load_dataset("csv", data_files='/content/treino_tweets.csv', delimiter='\t')
# test_dataset = load_dataset("csv", data_files='/content/test_tweets.csv', delimiter='\t')

train_dataset = pd.read_csv('/home/jjuliar/qanda/treino_tweets.csv', delimiter = '\t')
test_dataset = pd.read_csv('/home/jjuliar/qanda/test_tweets.csv', delimiter = '\t')

# tdf = pd.DataFrame({"a": [1, 2, 3], "b": ['hello', 'ola', 'thammi']})
# vdf = pd.DataFrame({"a": [4, 5, 6], "b": ['four', 'five', 'six']})

tdf = pd.DataFrame(train_dataset)
vdf = pd.DataFrame(test_dataset)
tds = Dataset.from_pandas(tdf)
vds = Dataset.from_pandas(vdf)
ds = DatasetDict()
ds['train'] = tds
ds['validation'] = vds
print(ds)

"""## Preprocess"""

tokenizer = AutoTokenizer.from_pretrained("PORTULAN/albertina-ptbr")

def preprocess_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

tokenized_train_dataset = ds['train'].map(preprocess_function, batched=True)
tokenized_test_dataset = ds['validation'].map(preprocess_function, batched=True)

tokenized_train_dataset.rename_columns({'prediction' : 'labels'})
tokenized_train_dataset = tokenized_train_dataset.rename_columns({'prediction' : 'labels'})

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

"""## Evaluate"""

accuracy = evaluate.load("accuracy")

"""Then create a function that passes your predictions and labels to [compute](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute) to calculate the accuracy:"""

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=labels)

eval_comb = evaluate.combine(["accuracy", "f1", "precision", "recall"])

"""Your `compute_metrics` function is ready to go now, and you'll return to it when you setup your training.

## Training

Before you start training your model, create a map of the expected ids to their labels with `id2label` and `label2id`:
"""

id2label = {0: "NEGATIVE", 1: "POSITIVE"}
label2id = {"NEGATIVE": 0, "POSITIVE": 1}

model = AutoModelForSequenceClassification.from_pretrained(
    "PORTULAN/albertina-ptbr", num_labels=2, id2label=id2label, label2id=label2id
)

training_args = TrainingArguments(
    output_dir="Claudiobot",
    learning_rate=1e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=3,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    push_to_hub=False,
    fp16=True,
    label_names = ["labels"],
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_test_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

# from google.colab import drive

# drive.mount('/content/gdrive')

trainer.save_model("Claudiobot")

"""## Inference"""

classifier = pipeline("text-classification", model="Claudiobot/")

"""##Results

"""

tokenized_test_dataset

from evaluate import evaluator

task_evaluator = evaluator("text-classification")

eval_results = task_evaluator.compute(
    model_or_pipeline=classifier,
    data=tokenized_test_dataset,
    metric=eval_comb,
    label_mapping={"NEGATIVE": 0, "POSITIVE": 1},
    label_column = 'label',
)

print(eval_results)

from transformers import EvalPrediction

# Assuming predictions and references are your predicted and reference values
preds = tokenized_test_dataset  # Your predicted values
labels = ds['validation']['label']  # Your reference values

# Create an instance of EvalPrediction
eval_prediction = EvalPrediction(predictions=preds, label_ids=labels)

# Calculate the balanced accuracy
balanced_accuracy = (eval_prediction.predictions.argmax(axis=1) == eval_prediction.label_ids).mean()

balanced_accuracy